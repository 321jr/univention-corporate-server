<chapter id="uvmm:chapter">
	<title>Virtualization</title>
	<section id="uvmm::introduction">
		<title>Introduction</title>
		<para>
			&ucsUVMM; (UVMM) is a management system for virtualization servers and virtual machines.
			It offers the possibility of monitoring all the virtualization servers registered in the UCS domain and administrating the virtual instances on these systems.
		</para>
		<para>
			A &ucsUMC; module forms the management interface.
			All virtualization servers are then administrated from this management system, see <xref linkend="uvmm::management"/>).
		</para>
		<para>
			<xref linkend="uvmm::installation"/> describes the installation of the management system and the virtualization servers and the functions of the &ucsUMC; module.
		</para>
		<para>
			In principle, the virtualized systems can run arbitrary operating systems.
			This mode of operation is called <emphasis>fully virtualized</emphasis> systems.
		</para>
		<para>
			Both virtualization technologies Xen and KVM are equally supported by &ucsUVMM;.
			However, the technologies have different advantages and disadvantages depending on the host systems and hardware used.
			For example, KVM requires virtualization support in the central processing unit, while Xen can also virtualize (within limits) systems without support from the hardware.
			More detailed information can be found on their respective websites:
			<ulink url="http://www.linux-kvm.org/"/> and <ulink url="http://www.xen.org/"/>.
		</para>
		<para>
			KVM and Xen each offer interfaces to provide the virtualized systems with direct access to the resources of the virtualization server.
			This considerably improves performance.
		</para>
		<para>
			In Xen, this technology is referred to as <emphasis>paravirtualization</emphasis>.
			KVM offers the <emphasis>virtIO</emphasis> interface.
			This allows network and storage devices a direct connection to the KVM resources.
			This technology is comparable to paravirtualization.
			When menu points in UVMM refer to paravirtualization, this also includes virtIO.
		</para>
		<para>
			UVMM supports both full virtualization and paravirtualization for virtualized systems.
			Using paravirtualization/virtIO is recommended.
		</para>
		<para>
			Current Linux systems support paravirtualization as standard.
			virtIO and Xen paravirtualization drivers for Microsoft Windows are included in UCS.
			The installation is documented in <xref linkend="uvmm::gplpvvirtio"/>.
		</para>
		<para>
			Univention Wiki (<ulink url="http://wiki.univention.de/"/>) includes a step-by-step quickstart guide <biblioref linkend="uvmm-quickstart"/> and further technical documentation as well as howtos <biblioref linkend="uvmm-technical-details"/>.
			(Currently only available in German)
		</para>
	</section>

	<section id="uvmm::installation">
		<title>Installation</title>
		<para>
			&ucsUVMM; comprises three different packages.
			They can all be selected directly when installing the UCS system or alternatively installed subsequently via &ucsUMC;.
		</para>
		<variablelist>
			<varlistentry>
				<term>
					Virtual Machine Manager (UVMM) (Package: univention-virtual-machine-manager-daemon)
				</term>
				<listitem>
					<simpara>
					This package must be installed on the management system.
					In doing so, both an additional service and the &ucsUMC; module are set up.
					This package should be installed on a domain controller in the UCS domain.
					The installation on &ucsMember;s requires additional configuration steps, which are documented in <biblioref linkend="uvmm-technical-details"/>.
					If UVMM is installed on a &ucsSlave; or &ucsBackup;, the package <emphasis>univention-virtual-machine-manager-schema</emphasis> needs to be installed on the &ucsMaster; beforehand.
					</simpara>
				</listitem>
			</varlistentry>
			<varlistentry>
				<term>
					Xen virtualization server (Package: univention-virtual-machine-manager-node-xen)
				</term>
				<listitem>
					<simpara>
					This package must be installed on each system which is to be used as a Xen-based virtualization server.
					</simpara>
				</listitem>
			</varlistentry>
			<varlistentry>
				<term>
					KVM virtualization server (Package: univention-virtual-machine-manager-node-kvm)
				</term>
				<listitem>
					<simpara>
					If KVM is to be used for the virtualization, this package should be installed on the virtualization servers.
					</simpara>
				</listitem>
			</varlistentry>
		</variablelist>
		<para>
			The two packages for the virtualization servers register the service in the LDAP directory.
			Additionally, a particular kernel is installed on Xen systems, which is necessary to allow Xen to be used.
		</para>
		<para>
			When installing the virtualization servers, only one virtualization technology should be used per server.
		</para>
		<para>
			Additionally, the architecture must also be taken into account during installation of the virtualization servers.
			64 bit systems can only be virtualized on UCS systems where are installed using the amd64 architecture.
			A 64-bit system (amd64) is recommended for use as the virtualization server.
		</para>
	</section>

	<section id="uvmm:imagefiles">
		<title>Image files of virtual machines</title>
		<para>
			If virtual hard drives are added to an instance, <emphasis>image files</emphasis> are usually used for the data keeping.
			An image file can either be generated for this purpose or an existing image file can be assigned to a virtual machine.
			Alternatively, a native block device (hard drive partition, logical volume, iSCSI volume) can be assigned to a virtual machine.
			The direct use of block devices offers performance advantages and is less susceptible to computer crashes.
		</para>
		<para>
			Hard drive images can be administrated in two ways on KVM systems;
			by default images are saved in the <emphasis>Extended format (qcow2)</emphasis>.
			This format supports Copy-on-write which means that changes do not overwrite the original version, but store new versions in different locations.
			The internal references of the file administration are then updated to allow both access to the original and the new version.
			This technique is a prerequisite for efficiently managing snapshots of virtual machines.
		</para>
		<para>
			Alternatively, you can also access a hard drive image in <emphasis>Simple format (raw)</emphasis>.
			Snapshots can only be created when using hard drive images in <emphasis>Extended format</emphasis>.
		</para>
		<para>
			Only the <emphasis>Simple format</emphasis> is available on Xen systems.
		</para>

		<para>
		  Operating systems use a so-called <emphasis>page cache</emphasis> to accelerate accesses
		  to storage media. If data are accessed which have already been read off a hard drive and
		  these data are still present in the cache, the comparatively slow access to the storage
		  medium is not necessary and the request is answered directly from the page cache.
		</para>

		<para>
		  Write accesses are generally also not directly written on the hard drive, but are usually
		  bundled and, consequently, written more efficiently. However, this involves the risk of
		  data loss, if, for example, a system crashes or the power supply is interrupted. The data
		  which have been only saved in the write cache up to that point and have yet to be
		  synchronised on the storage medium are lost. For this reasons, modern operating systems
		  generally only keep pending write changes for a maximum of several seconds before writing
		  them to the hard drive.
		</para>

		<para>
		  In order to avoid data being stored doubly in the page cache of the host system and also
		  of the guest system, cache strategies can be configured with the
		  <guimenu>Caching</guimenu> option when using KVM, which influence the use of the host
		  system's page cache:
		</para>

		<itemizedlist>
			<listitem>
				<simpara>
			  The default setting since UCS-3.1 is <emphasis>none</emphasis>: in this setting, KVM
			  accesses the hard drive directly and bypasses the page cache on the virtualization
			  server. Read accesses are answered directly by the hard drive every time and write
			  accesses are passed directly on to the hard drive.
				</simpara>
			</listitem>

			<listitem>
				<simpara>
			  The <emphasis>write-through</emphasis> strategy uses the page cache on the
			  virtualization server, but every write access is also passed on directly to the
			  storage medium. On virtualization servers with a lot of free system memory, read
			  accesses can be more efficient than <emphasis>none</emphasis>. However, the double
			  caching generally has a negative effect on the overall performance.

			  <footnote><para>Instead, it is recommended to make the free memory directly available
			  to the VMs so that they can use the additional memory more efficiently themselves, for
			  instance for caching.</para></footnote>
				</simpara>
			</listitem>

			<listitem>
				<simpara>
			  If the <emphasis>write-back</emphasis> strategy is used, the host's page cache will
			  be used for both read and write accesses. Write accesses are initially only performed
			  in the page cache, before they are then written to the hard drive at a later point in
			  time. In this case, if the host system crashes, data may be lost.
				</simpara>
			</listitem>

			<listitem>
				<simpara>
			  With the <emphasis>unsafe</emphasis> strategy, synchronisation requests sent by the
			  guest system are ignored in order to force the writing of outstanding data on the
			  storage medium explicitly. Compared with <emphasis>write-back</emphasis>, this once
			  again increases the performance, but can result in data loss if the host system
			  crashes. This version is only practical for test systems or comparable installations
			  in which data loss due to the crashing of the host system is not dramatic.
				</simpara>
			</listitem>

			<listitem>
				<simpara>
			  The <emphasis>directsync</emphasis> strategy corresponds to <emphasis>none</emphasis>,
			  with the only difference being that here synchronisation is explicitly forced after
			  every write access.
				</simpara>
			</listitem>

			<listitem>
				<simpara>
			  The <emphasis>Hypervisor default</emphasis> option is dependent on the UCS version and
			  the KVM version with which a guest system was installed: Originally, the standard
			  value until UCS 3.0 was implicitly <emphasis>write-through</emphasis>, but KVM was
			  modified to such an extent with UCS 3.1 that <emphasis>none</emphasis> is now used for
			  all old VMs instead. For VMs resaved with UCS 3.1 the standard value is implicitly
			  <emphasis>write-through</emphasis> again, but new VMs are explicitly saved with
			  <emphasis>none</emphasis>.
				</simpara>
			</listitem>
		</itemizedlist>

		<para>
		  Xen uses its own system called Tapdisk, which bypasses the guest's page cache similarly
		  to <emphasis>none</emphasis>.
		</para>

		<para>
			These image files are stored in so-called storage pools. They can either be stored
			locally on the virtualization server or on a file share.
		</para>
		<para>
			If a live migration of virtual machines between different virtualization servers is planned, the storage pool must be stored on a system which can be accessed by all virtualization servers (e.g., an NFS share or an iSCSI target).
			This is described in <biblioref linkend="uvmm-technical-details"/>.
		</para>
		<para>
			Image files are created as sparse files with the specified size, i.e., these files only grow when they are used and then up to the maximally specified size and thus initially require only minimal disk space.
			As there is a risk here of the disk space being used up during operation, a Nagios monitoring should be integrated, see <xref linkend="nagios::general"/>.
		</para>
	</section>

	<section id="uvmm::defaultpool">
	  <title>Accessing the default storage pool through a file share</title>
		<para>
		  Each virtualization server provides a storage pool with the name
		  <emphasis>default</emphasis> in the standard configuration. It can be found on the
		  virtualization servers in the <filename>/var/lib/libvirt/images/</filename> directory.
		</para>

		<para>
		  To allow simple access to the storage pool, you can set up a share for the
		  <filename>/var/lib/libvirt/images/</filename> directory. To do so, you need to create a
		  share with the following options in the UMC module <guimenu>Shares</guimenu>. The share
		  can then be accessed easily from Windows clients via a CIFS network share (or via an NFS
		  mount).
		</para>
		<itemizedlist>
			<listitem>
				<para>General/General settings</para>
				<itemizedlist>
					<listitem><simpara>Name: UVMM-Pool</simpara></listitem>
					<listitem><simpara>Host: Der Rechnername des UVMM-Servers</simpara></listitem>
					<listitem><simpara>Directory: /var/lib/libvirt/images</simpara></listitem>
					<listitem><simpara>Directory owner, Directory owner group and Directory mode can remain in the default setting</simpara></listitem>
				</itemizedlist>
			</listitem>
			<listitem>
				<para>Advanced settings/Samba permissions</para>
				<itemizedlist>
					<listitem><simpara>Valid users or groups: Administrator</simpara></listitem>
				</itemizedlist>
			</listitem>
		</itemizedlist>
		<para>
		  The image files of a virtual hard drive include all the user data of the virtualized
		  system! The <guimenu>Valid users or groups</guimenu> option ensures that, irrespective
		  of the file system permissions, only the Administrator user can access the share.
		</para>
	</section>

	<section id="uvmm::drives">
		<title>CD/DVD/floppy drives in virtual machines</title>
		<para>
			CD-/DVD-ROM/floppy drives can be mounted in two ways:
		</para>
		<itemizedlist>
			<listitem>
				<simpara>
				An ISO image can be assigned from a storage pool. If no additional storage pool has
				been created, the files from the pool <emphasis>default</emphasis> are read from the
				directory <filename>/var/lib/libvirt/images/</filename>.
				</simpara>
			</listitem>
			<listitem>
				<simpara>
				Alternatively, a physical drive from the virtualization server can be connected with the virtual machine.
				</simpara>
			</listitem>
		</itemizedlist>
		<para>
			It is also possible to provide a virtual machine with a disk drive via an image (in VFD format) or the pass-through of a physical drive.
		</para>
		<para>
			If drives are defined for a new virtual machine, it must be ensured that it is possible to boot from the CD-ROM drive.
			The UVMM profile specifies the boot order for the fully-virtualized instances in advance.
			For the paravirtualized instances, it is defined by the order on the definition of the drives and can be adapted subsequently in the settings section.
		</para>
	</section>

	<section id="uvmm:networkinterfaces">
		<title>Network interfaces in virtual instances</title>
		<para>
			When a virtual machine is created, it is automatically assigned a network card with a randomly generated MAC address.
			It can be subsequently changed.
		</para>
		<para>
			Two types of network connections are possible:
		</para>
		<itemizedlist>
			<listitem>
				<simpara>
				In the basic settings, a <emphasis>Bridge</emphasis> on the virtualization server is used to access the network directly.
				The virtual machine uses its own IP address and can thus also be reached from other computers.
				</simpara>
			</listitem>
			<listitem>
				<simpara>
				<emphasis>Network Address Translation(NAT)</emphasis> network cards are defined in a private network on the virtualization server.
				To do so, the virtual machine(s) must be assigned an IP address from the 192.168.122.0/24 network.
				This virtual instance is granted the access to the external network via NAT, so that the access is performed via the virtualization server's IP address.
				The virtual machine can thus not be reached from other computers, but can create all outgoing connections itself.
				</simpara>
			</listitem>
		</itemizedlist>
		<figure id="uvmm-network">
			<title>Adding a virtual network interface</title>
			<graphic scalefit="1" width="40%" fileref="illustrations/uvmm_network_en.png"/>
		</figure>
		<para>
			The UVMM servers are already preconfigured for bridging and NAT.
			However, there are restrictions for bridged network cards.
			On the UVMM servers, the physical network card to which the standard route is set, is converted to a bridge in the default setting.
			If additional network cards are integrated in the server, these are not adapted accordingly.
			If several bridge network cards are required in a virtual instance, an additional network card must be configured as a bridge on the server in advance.
			If a bridge is used, the <guimenu>Source</guimenu> of the network interface used can be selected.
		</para>
		<para>
			NAT network cards are only restricted by the IP addresses available in the 192.168.122.0/24 network.
		</para>
		<para>
			The <guimenu>Driver</guimenu> can be used to select what type of card will be provided.
			The <emphasis>Realteak RTL-8139</emphasis> is supported by almost all operating systems, the <emphasis>Intel Pro-1000</emphasis> offers advanced abilities and a <emphasis>Paravirtual device</emphasis> offers the best performance.
		</para>
	</section>

	<section id="uvmm::gplpvvirtio">
		<title>Paravirtualization/virtIO drivers for Microsoft Windows systems</title>
		<para>
			virtIO and Xen paravirtualization drivers for Microsoft Windows are included in UCS.
			Installing the respective KVM or Xen packages provides appropriate images, which can be added in the drive settings of a virtual machine.
			The images are added to the storage pool specified with the &ucsUCRV; <envar>uvmm/pool/default/path</envar>:
		</para>
		<itemizedlist>
			<listitem>
				<simpara>
				On Xen virtualization servers an ISO image named <emphasis>Xen Windows drivers (gplpv 308)</emphasis> is provided, which contains the GPLPV virtualization driver for Microsoft Windows.
				</simpara>
			</listitem>
			<listitem>
				<simpara>
				On KVM virtualization servers an ISO and a floppy image named <emphasis>KVM Windows drivers (virtio 1.1.16)</emphasis> are provided, which contains the virtIO virtualization driver for Microsoft Windows.
				</simpara>
			</listitem>
		</itemizedlist>
		<section id="uvmm:Installation_of_the_GPLPV_drivers_for_Xen_instances">
			<title>Installation of the GPLPV drivers for Xen instances</title>
			<para>
				The GPLPV driver is a open source driver for Microsoft Windows, which enables Windows DomU systems virtualized in Xen to efficiently access the network and storage resources of the Xen Dom0.
				This provides a significant performance and reliability gain over the emulated standard devices.
			</para>
			<para>
				Univention provides the GPLPV drivers signed with a Software Publishers Certificate obtained from the VeriSign CA.
			</para>
			<para>
				There are different MSI installer packages for the various Windows releases, which can be started with a simple double click.
				The <guimenu>typical</guimenu> installation variant should cover most use cases.
				The Windows installations need to be updated to the current service packs before installing the GPLPV drivers.
				E.g., using the GPLPV driver on Windows XP w/o SP3 is not possible.
			</para>
			<table>
				<title>Windows virtualization drivers</title>
				<tgroup cols="2">
					<colspec colnum="1" colname="col1" colwidth="2*"/>
					<colspec colnum="2" colname="col2" colwidth="1*"/>
					<thead>
						<row>
							<entry>File name of driver package</entry>
							<entry>Windows version</entry>
						</row>
					</thead>
					<tbody>
					  <row>
						<entry>gplpv_2000_signed_0.11.0.372.msi</entry>
						<entry>Windows 2000</entry>
					  </row>
					  <row>
						<entry>gplpv_2003x32_signed_0.11.0.372.msi</entry>
						<entry>Windows 2003 (32 Bit)</entry>
					  </row>
					  <row>
						<entry>gplpv_2003x64_signed_0.11.0.372.msi</entry>
						<entry>Windows 2003 (64 Bit)</entry>
					  </row>
					  <row>
						<entry>gplpv_Vista2008x32_signed_0.11.0.372.msi</entry>
						<entry>Windows Vista (32 Bit)</entry>
					  </row>
					  <row>
						<entry>gplpv_Vista2008x32_signed_0.11.0.372.msi</entry>
						<entry>Windows 2008 (32 Bit)</entry>
					  </row>
					  <row>
						<entry>gplpv_Vista2008x64_signed_0.11.0.372.msi</entry>
						<entry>Windows Vista (64 Bit)</entry>
					  </row>
					  <row>
						<entry>gplpv_Vista2008x64_signed_0.11.0.372.msi</entry>
						<entry>Windows 2008 (64 Bit)</entry>
					  </row>
					  <row>
						<entry>gplpv_XP_signed_0.11.0.372.msi</entry>
						<entry>Windows XP (32 Bit)</entry>
					  </row>
					</tbody>
				</tgroup>
			</table>
			<para>
				After successful installation and a reboot <emphasis>Xen Net Device Driver</emphasis> and <emphasis>Xen Block Device Driver</emphasis> can be found in the device manager.
			</para>
		</section>

		<section id="uvmm:Installation_of_the_virtIO_drivers_for_KVM_instances">
			<title>Installation of the virtIO drivers for KVM instances</title>
			<para>
				In Windows systems installed under KVM, paravirtualization must be activated <emphasis>before</emphasis> beginning the Windows installation.
			</para>
			<para>
				The virtIO interface allows the efficient usage of network and storage resources for a virtual machine on the KVM hypervisor.
				The following steps describe the installation of the virtIO drivers on Windows 7.
			</para>
			<itemizedlist>
				<listitem>
					<simpara>
					A floppy drive needs to be setup in the drive settings with the image <emphasis>virtio 1.1.16.vfd</emphasis> assigned.
					</simpara>
				</listitem>
				<listitem>
					<simpara>
					The hard disk drive has to be edited in the <guimenu>Devices</guimenu> menu in UVMM and the checkbox <guimenu>Paravirtual drive</guimenu> must be ticked.
					</simpara>
				</listitem>
				<listitem>
					<simpara>
					The initial steps during the installation of the Windows system take place as usual.
					A warning appears during hard disk partitioning and states that no mass storage could be found.
					This is not an error because the virtIO drivers are necessary for a paravirtualized device.
					The virtIO drivers can be installed in the same menu with <guimenu>Load drivers</guimenu>.
					The <emphasis>Red Hat virtIO SCSI Controller</emphasis> has to be chosen for Windows 7 (and for Windows 2003 and Windows 2008 respectively) and the <emphasis>Red Hat virtIO Ethernet Adapter</emphasis> for Windows 2008/Windows 7.
					After the device drivers have been installed, the mass storage is available in the Windows installer and the installation of Microsoft Windows can be continued.
					</simpara>
				</listitem>
				<listitem>
					<simpara>
					After completing the installation the devices <emphasis>Red Hat virtIO SCSI Disk Device</emphasis> and <emphasis>Red Hat virtIO Ethernet Adapter</emphasis> can be found in the Windows device manager.
					</simpara>
				</listitem>
			</itemizedlist>
		</section>
	</section>

	<section id="uvmm::snapshots">
		<title>Snapshots</title>
		<para>
			UVMM offers the possibility to save the contents of the main and hard drive memory of a virtual machine in snapshots.
			This allows the administrator to revert to these snapshots at a later point in time, which makes them a useful "safety net" when installing software updates.
		</para>
		<para>
			Snapshots can only be used with KVM instances which access all their hard drive images in Qcow2 format.
			All snapshots are stored using copy-on-write (see <xref linkend="uvmm::instanz::erstellen"/>) directly in the hard drive image file.
		</para>
	</section>

	<section id="uvmm:migration">
		<title>Migration of virtual instances</title>
		<para>
			UVMM offers the the possibility of migrating a virtual machine to another virtualization server.
			This works with both paused and running instances (live migration).
			The option is only offered if at least two compatible virtualization servers are available in the domain.
		</para>
		<figure id="uvmm-migrate">
			<title>Migrating a virtual instance</title>
			<graphic scalefit="1" width="40%" fileref="illustrations/uvmm_migrate_en.png"/>
		</figure>
		<para>
			During the migration it must be noted that the images of the mounted hard drives and CD-ROM drive must be accesible by both virtualization servers.
			This can be achieved, for example, by storing the images in a central storage system.
			Notes on the setting up of this type of environment can be found under <biblioref linkend="uvmm-technical-details"/>.
		</para>
		<section id="uvmm:Migration_of_virtual_machines_from_failed_virtualization_servers">
			<title>Migration of virtual machines from failed virtualization servers</title>
			<para>
				Information about the virtual machines running on the virtualization servers is stored centrally in the &ucsUVMM;.
				If a server fails (failure detection is performed periodically every 15 seconds), the server and the virtual instances operated on it are identified as unaccessible with a red symbol, a warning appears and <guimenu>Migrate</guimenu> is offered as the only operation in the menu.
			</para>
			<para>
				Following the migration, the virtual instance is no longer displayed in the overview tree of the failed virtualization server in the UVMM.
			</para>
			<caution>
				<para>
				It must be ensured under all circumstances that the virtual machine on the original and the secondary server are not started in parallel; this would involve their both writing in the image files simultaneously, which would result in data loss.
				If virtual machines are started automatically after startup, simultaneous access must be prohibited by disconnecting the network connection or restricting access to the storage pool.
				</para>
			</caution>
			<para>
				If the failed computer is reactivated - e.g., in the case of a temporary power failure - the virtual machines remain available on the system locally and are reported to UVMM; consequently, there are then two versions of the instance.
			</para>
			<para>
				As such, one of the two instances should subsequently be deleted.
				However, the employed image files for the drives should <emphasis>not</emphasis> be deleted at the same time.
			</para>
		</section>
	</section>

	<section id="uvmm::management">
		<title>Managing virtual machines with the Univention Management Console</title>
		<para>
			The &ucsUMC; module <emphasis>Virtual machines (UVMM)</emphasis> offers the possibility to create, edit and delete virtual instances/machines and to change their status.

			In principle, these functions are independent of the virtualization technology employed (Xen or KVM), however they may vary slightly depending on the hypervisor in use.
			The items that must be observed are illustrated in the following section on the description of the functions.
		</para>

		<section id="uvmm::overview">
			<title>Operations (Starting/stopping/suspending/deleting/migrating/cloning virtual machines)</title>
			<figure id="uvmm-overview">
				<title>Overview of virtual machines</title>
				<graphic scalefit="1" width="80%" fileref="illustrations/uvmm_overview1_en.png"/>
			</figure>
			<para>
				In the main dialog of the UMC module, a tree structure is displayed on the left-hand side, which gives an overview of the existing virtualization servers.
				All the virtual machines are listed in the right half of the screen.
				If one clicks on the name of a virtualization server, only the instances of that server are listed.
				The search mask can also be used to search for individual virtual machines.
			</para>
			<para>
				In the overview of the virtual machines, the computer icon shows the state this is in, e.g., whether it is running (computer symbol with green arrow), paused (computer symbol with yellow line) or stopped (computer without additional symbol).
			</para>
			<para>
				Instances created via UVMM are turned off in the initial status.
				The icon showing an arrow pointing right can be used to start a virtual instance.
			</para>
			<para>
				Running instances can be accessed via the VNC protocol - insofar as this is configured.
				The icon with the stylised screen opens a connection with a Java-based client.
				Any other VNC client can also be used for the access;
				the VNC port is displayed in a tooltip above the computer name.
			</para>
			<para>
				The <guimenu>more</guimenu> choice box can be used to perform other operations:
				The following operations are available on running instances:
			</para>
			<variablelist>
				<varlistentry>
					<term>Stop</term>
					<listitem>
						<simpara>
						turns the virtual machine off.
						It must be noted that the operating system of the virtual machine is not shutdown first, i.e., it should be compared with turning off a computer by pulling the power plug.
						</simpara>
					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Pause</term>
					<listitem>
						<simpara>
						assigns the instance no further CPU time.
						This still uses the working memory on the virtualization server, but the instance itself is paused.
						</simpara>
					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Suspend</term>
					<listitem>
						<simpara>
						saves the contents of the machine's system memory on the hard drive and does not assign the machine further CPU time, i.e., compared with <guimenu>Pause</guimenu> the working memory is also freed.
						This function is only available on KVM-based virtualization servers.
						</simpara>
					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Migrate</term>
					<listitem>
						<simpara>
						migrates the virtual machine to another virtualization server.
						Further information can be found in <xref linkend="uvmm:migration"/>.
						</simpara>
					</listitem>
				</varlistentry>
			</variablelist>
			<para>
				The following operations are available on saved or stopped instances:
			</para>
			<variablelist>
				<varlistentry>
					<term>Remove</term>
					<listitem>
						<simpara>
						Virtual instances no longer required can be deleted along with all their hard drives and ISO images.
						The images to be deleted can be selected from a list.
						It must be noted that ISO images and sometimes also hard drive images may still be used by other instances.
						They should only be deleted when they are no longer used by any instance.
						</simpara>
					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Migrate</term>
					<listitem>
						<simpara>
						migrates the virtual machine to another virtualization server.
						Further information can be found in <xref linkend="uvmm:migration"/>.
						</simpara>
					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Clone</term>
					<listitem>
						<simpara>
						creates a copy of the current VM. It is given a freely selectable,
					new name. Network interfaces are adopted, but can also alternatively be randomly
					regenerated. Mounted CD and DVD drives from the source VM are also integrated in
					the clone, while hard drives are copied insofar as the storage pool supports the
					copying. Snapshots are not copied!
						</simpara>
					</listitem>
				</varlistentry>
			</variablelist>
		</section>

		<section id="uvmm::instanz::erstellen">
			<title>Creating a virtual instance</title>
			<para>
				Virtual machines can be created with the assistant in a few steps in UVMM by clicking on <guimenu>Create virtual instance</guimenu>.
			</para>
			<para>
				The input mask <guimenu>Physical server</guimenu> can be used to select on which virtualization server the virtual machine should be created.
				The selection of the <guimenu>Profile</guimenu> specifies some of the basic settings for the virtual instance (e.g., a name prefix, no. of CPUs, RAM and whether the direct access per VNC should be activated).
			</para>
			<para>
				The existing UVMM profiles are stored in the LDAP directory and can also be edited there.
				The profiles can be found in the <guimenu>LDAP directory</guimenu> section of the &ucsUMC; in the container <emphasis>cn=Profiles,cn=Virtual Machine Manager</emphasis>.
				Additional profiles can also be added there.
			</para>
			<para>
				The virtual machine is now given a <guimenu>Name</guimenu> and an optional <guimenu>Description</guimenu> and assigned <guimenu>Memory</guimenu> and <guimenu>CPUs</guimenu>.

				The <guimenu>Enable direct access</guimenu> option specifies whether the machine can be accessed via the VNC protocol.
				This is generally required for the initial operating system installation.
			</para>
			<para>
				Now the disk drives of the virtual machines are configured.
				The setup is documented in <xref linkend="uvmm:imagefiles"/>.
			</para>
			<para>
				Clicking <guimenu>Finish</guimenu> concludes the creation of the virtual machine.
		</para>
	</section>

	<section id="uvmm-instance-edit">
		<title>Modifying virtual machines</title>
		<para>
			In the overview list, a virtual machine can be edited by clicking on the icon with the stylized pen.
		</para>
		<figure id="uvmm-drive">
			<title>Modifying the settings of a DVD drive</title>
			<graphic scalefit="1" width="30%" fileref="illustrations/uvmm_dvd_en.png"/>
		</figure>
			<table>
				<title>'General' tab</title>
				<tgroup cols="2">
					<colspec colnum="1" colname="col1" colwidth="1*"/>
					<colspec colnum="2" colname="col2" colwidth="2*"/>
					<thead>
						<row>
							<entry>Attribut</entry>
							<entry>Description</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>Name</entry>
							<entry>
								Defines the name of the virtual machine.
								This does not have to be the same as the name of the host in the LDAP directory.
							</entry>
						</row>
						<row>
							<entry>Operating system</entry>
							<entry>
								The operating system installed in the virtual instance.
								Any text can be entered here.
							</entry>
						</row>
						<row>
							<entry>Contact</entry>
							<entry>
								Defines the contact person for the virtual machine.
								If an e-mail address is specified here, an external e-mail program can then be run via the mouseover that appears.
							</entry>
						</row>
						<row>
							<entry>Description</entry>
							<entry>
							  Can be used to describe the function of the virtual machine,
							  e.g. <emphasis>mail server</emphasis> or it's state. The description
							  is shown in the overview of the virtual machines as a mouseover.
							</entry>
						</row>
					</tbody>
				</tgroup>
			</table>

			<para>
			  The tab <guimenu>Devices</guimenu> allows the
			  configuration of drives and network interfaces. An
			  introduction to the supported devices, image formats and
			  storage pools can be found in the <xref
			  linkend="uvmm:imagefiles"/>. An introduction to the
			  supported network card settings can be found in the
			  <xref linkend="uvmm:networkinterfaces"/>.
			</para>
			<para>
				<guimenu>Drives</guimenu> lists all existing drives, the image files used, their size and the assigned storage pools.
				One can click on the stylised minus sign to delete a drive and <guimenu>Edit</guimenu> can be used to adjust setting subsequently.
			</para>
			<para>
				<guimenu>Paravirtual drive</guimenu> allows specification of whether the access to the drive should be paravirtualized.
				Where possible, this setting should not be changed for a virtual machine which already has an operating system installed, as this may disrupt the access of partitions.

			</para>
			<para>
				If drives or network interfaces are subsequently added to a virtual instance, the utilisation of paravirtualization is determined by heuristics or its profile.
			</para>
			<para>
				<guimenu>Add drive</guimenu> can be used to add an additional drive.
			</para>
			<para>
				This menu contains a list of all network cards; in addition, new cards can be added or existing ones edited.
				<guimenu>Add network interface</guimenu> can be used to add another virtual network card.
			</para>

			<para>
			  The tab <guimenu>Snapshots</guimenu> contains a list of all available snapshots.
			  An introduction to snapshots can be found in the <xref linkend="uvmm::snapshots"/>.
			</para>
			<para>
				<guimenu>Snapshots</guimenu> includes a list of all the existing snapshots.
				<guimenu>Resume</guimenu> can be used to restore an earlier status.
			</para>
			<caution>
				<para>
				The current machine state is lost if the old snapshot is restored.
				However, there is no reason not to save the current state in an additional snapshot in advance.
				</para>
			</caution>
			<para>
				A snapshot can be removed by clicking in the stylised minus sign.
				The current state of the virtual machine is not modified by this.
			</para>
			<para>
				<guimenu>Create new snapshot</guimenu> can be used to create a snapshot with the name of your choice, e.g., <emphasis>DC Master before update to UCS 2.4-2</emphasis>.
				In addition to the description the time is saved when the snapshot is created.
			</para>

			<para>
			  The settings of a virtual machine can only be changed if it is turned off.
			</para>

			<table>
				<title>'Advanced' tab</title>
				<tgroup cols="2">
					<colspec colnum="1" colname="col1" colwidth="1*"/>
					<colspec colnum="2" colname="col2" colwidth="2*"/>
					<thead>
						<row>
							<entry>Attribute</entry>
							<entry>Descrition</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>Architecture</entry>
							<entry>
								Specifies the architecture of the emulated hardware.
								It must be noted that virtual 64 bit machines can only be created on virtualization servers using the amd64 architecture.
								This setting is not shown on i386 and Xen systems.
							</entry>
						</row>
						<row>
							<entry>Number of CPUs</entry>
							<entry>
								Defines how many CPU sockets are assigned to the virtual instance.
								The number of NUMA nodes, cores and CPU threads is not currently configurable.
							</entry>
						</row>
						<row>
							<entry>Memory</entry>
							<entry>
								Specifies the size of the assigned system memory.
							</entry>
						</row>
						<row>
							<entry>Virtualization technology</entry>
							<entry>
								The technology used for virtualization.
								This setting can only be specified when creating a virtual instance.
							</entry>
						</row>
						<row>
							<entry>RTC reference</entry>
							<entry>
								In fully virtualized systems, a computer clock is emulated for each virtual machine (paravirtualized systems access the clock on the host system directly).
								This option controls the format of the emulated clock;
								it an either be saved in the <guimenu>coordinated universal time (UTC)</guimenu> or the <guimenu>local timezone</guimenu>.
								The use of UTC is recommended for Linux system and the use of the local time zone recommended for Microsoft Windows systems.
							</entry>
						</row>
						<row>
							<entry>Boot order</entry>
							<entry>
								Specifies the order in which the emulated BIOS of the virtual machine searches the drives for bootable media.
								This setting is only available for fully-virtualized instances.
								On paravirtualized machines it is only possible to select one hard drive from which the kernel should be used.
							</entry>
						</row>
						<row>
							<entry>Direct access (VNC)</entry>
							<entry>
								Defines whether VNC access to the virtual machine is available.
								If the option is activated, the UMC module can be used to start a VNC program directly.
								The VNC URL is displayed in a tool tip.
								A Java VNC program is used for this in the default setting.
							</entry>
						</row>
						<row>
							<entry>Globally available</entry>
							<entry>
								This allows VNC access from other systems than the virtualization server.
							</entry>
						</row>
						<row>
							<entry>VNC Password</entry>
							<entry>
								Sets a password for the VNC connection.
							</entry>
						</row>
						<row>
							<entry>Keyboard layout</entry>
							<entry>
								Defines the layout for the keyboard in the VNC session.
							</entry>
						</row>
					</tbody>
				</tgroup>
			</table>
		</section>
	</section>
</chapter>
