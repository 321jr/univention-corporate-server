<chapter id="uvmm:chapter">
	<title>Virtualization</title>
<!-- TODO: translation
	<section id="uvmm::introduction">
		<title>Introduction</title>
		<para>
			&ucsUVMM; (UVMM) ist ein Werkzeug für die Verwaltung hybrider Cloud-Umgebungen. Es können
			in der UCS-Domäne regiustrierte KVM-Virtualisierungsserver und darauf betriebene 
			virtuelle Maschinenen zentral überwacht und administrtiert werden. Zusätzlich können
			virtuelle Maschinen in OpenStack- oder EC2-Umgebungen administriert werden.
			Die Administration erfolgt über das &ucsUMC;-Modul <emphasis>Virtuelle Maschinen</emphasis>.
		</para>
		<para>
			In den virtualisierten Systemen kann im Prinzip jedes beliebige Betriebssystem verwendet werden.
		</para>

	</section>
-->

	<section id="uvmm::installation">
		<title>Installation</title>

		<para>
		  Univention Virtual Machine Manager can be installed from the Univention App Center with the application
		  <emphasis>Management server for KVM and cloud instances</emphasis>. Alternatively, the software package
		  <package>univention-virtual-machine-manager-daemon</package> can be installed.
		  Additional information can be found in
		  <xref linkend="computers::softwaremanagement::installsoftware"/>.
		</para>
<!-- TODO: Move figure into correct place -->
			<figure id="uvmm-new-openstack-connection">
				<title>Creating a new connection to an OpenStack instance</title>
				<graphic scalefit="1" width="90%" fileref="illustrations/uvmm_new_openstack_connection_en.png"/>
			</figure>

<!-- TODO: translate
		<para>
			Die Verwaltung von OpenStack Cloud-Instanzen ist direkt nach der Installation der Applikation mit dem &ucsUMC; Modul <emphasis>Virtuelle Maschinen (UVMM)</emphasis> möglich.
			Für die Verwaltung von virtuellen Maschinen in der Amazon EC2 Cloud muss die Applikation <emphasis>Amazon EC2 Cloud-Verbindung</emphasis> installiert werden.
		</para>
		<para>
			Um vor Ort einen KVM Virtualisierungsserver für die Verwaltung durch &ucsUVMM; hinzuzufügen, muss die Applikation <emphasis>KVM Virtualisierungsserver</emphasis> aus dem Univention App Center auf einem Server der Domäne installiert werden.
			Die Applikation kann auch direkt bei der Installation eines neuen UCS Servers ausgewählt werden.
			Alternativ kann das Softwarepaket <package>univention-virtual-machine-manager-node-kvm</package> installiert werden.
		</para>
		<para>
			Zum Betrieb von KVM wird zwingend CPU-Virtualisierungunterstützung benötigt.
			Diese wird von nahezu allen aktuellen x86 CPUs bereitgestellt.
			Zu Details kann die Webseite des KVM Projekts konsultiert werden:
			<ulink url="http://www.linux-kvm.org/"/>.
		</para>

		<para>
			Zusätzlich sollte bei der Installation eines Virtualisierungsservers die Architektur beachtet werden.
			Nur auf UCS-Systemen, die mit der amd64-Architektur installiert sind, können auch 64-Bit-Systeme virtualisiert werden.
			Für den Einsatz als Virtualisierungsserver wird die Verwendung eines 64-Bit-Systems (amd64) empfohlen.
		</para>
	<section id="uvmm::cloudconnections">
		<title>Anlegen von Verbindungen zu Cloud Computing Instanzen</title>
		<para>
			&ucsUVMM; unterstützt Verbindungen zu OpenStack.
			Durch die Installation der Applikation <emphasis>Amazon EC2 Cloud-Verbindung</emphasis> ist auch das Verwalten von virtuellen Maschinen in der Amazon EC2 Cloud möglich.
		</para>
		<para>
			Um eine neue Verbindung anzulegen, muss das &ucsUMC; Modul <emphasis>Virtuelle Maschinen (UVMM)</emphasis> geöffnet werden.
			Durch einen Klick auf <guimenu>Erstellen</guimenu> öffnet sich ein Assistent, in dem der Punkt <guimenu>Erstellen einer neuen Cloud-Verbindung</guimenu> gewählt werden muss.
			Im nun verfügbaren Dropdown Feld kann die Art der Verbindung gewählt werden, mit einem Klick auf <guimenu>Weiter</guimenu> startet der Einrichtungsassistent.
			Sind die Einstellungen getätigt, wird mit einem Klick auf <guimenu>Fertigstellen</guimenu> versucht, die Verbindung herzustellen.
			Falls ein Fehler auftritt, wird dieser angezeigt und die Verbindungseinstellunge können korrigiert werden.
			Wenn die Verbindung erfolgreich hergestellt wurde, wird eine Warteanimation angezeigt, während alle verbindungsspezifischen Informationen der Cloud Verbindung geladen werden.
			Dies umfasst zum Beispiel die vorhandenen Instanzen und verfügbare Images, um neue Instanzen anlegen zu können.
		</para>
		<section id="uvmm:cloudconnections::openstack">
			<title>Anlegen einer OpenStack Verbindung</title>
			<para>
				Um eine Verbindung zu einer OpenStack Instanz herzustellen, sind im Einrichtungsassistenten folgende Einstellungen vorzunehmen:
				<table>
					<title>Felder bei der Einrichtung einer OpenStack Verbindung</title>
					<tgroup cols="2">
						<colspec colnum="1" colname="col1" colwidth="1*"/>
						<colspec colnum="2" colname="col2" colwidth="2*"/>
						<thead>
							<row>
								<entry>Attribut</entry>
								<entry>Beschreibung</entry>
							</row>
						</thead>
						<tbody>
							<row>
								<entry>Name</entry>
								<entry>
									Definiert den Namen der Verbindung.
									Dieser wird später in der Baumansicht des &ucsUMC; Moduls angezeigt.
								</entry>
							</row>
							<row>
								<entry>Benutzername</entry>
								<entry>
									Der Benutzername, der zur Authentifizierung an OpenStack benutzt werden soll.
								</entry>
							</row>
							<row>
								<entry>Folgenden Authentifizierungstyp nutzen</entry>
								<entry>
									Es kann zwischen zwei Werten gewählt werden. Der zugehörige Wert wird im darunterliegenden Feld eingegeben.
									<variablelist>
										<varlistentry>
											<term>Passwort</term>
											<listitem>
												<simpara>
													Das zum Benutzernamen gehörige Passwort.
												</simpara>
											</listitem>
										</varlistentry>
										<varlistentry>
											<term>API-Schlüssel</term>
											<listitem>
												<simpara>
													Der API-Schlüssel, der dem Benutzer Zugriff verschafft.
												</simpara>
											</listitem>
										</varlistentry>
									</variablelist>
								</entry>
							</row>
							<row>
								<entry>URL des Authentifizierungs-Endpunktes</entry>
								<entry>
									Hier ist die URL einzutragen, unter der der Authentifizierungs-Endpunkt der OpenStack Instanz erreichbar ist.
									Soll eine verschlüsselte Verbindung aufgebaut werden, ist die URL in der Form <emphasis>https://[...]</emphasis> anzugeben.
									Da für die verschlüsselte Verbindung das öffentliche Zertifikat der OpenStack Instanz verwendet wird, muss dieses dem UVMM System bekannt gemacht werden.
									Auf dem UCS System, auf dem die Applikation <emphasis>Management-Server für KVM und Cloud-Instanzen</emphasis> installiert ist, muss dieses Zertifikat verfügbar gemacht werden.
									Dazu muss das öffentliche Zertifikat in PEM Kodierung auf dem UCS Server in das Verzeichnis <filename class="directory">/usr/local/share/ca-certificates/</filename> kopiert werden und mit der Endung .crt versehen sein.
									Die folgenden Kommandos konvertieren ein Zertifikat in die korrekte Kodierung und machen das Zertifikat bekannt:
									<programlisting language="sh">
openssl x509 -in [pfad/zum/openstack-zertifikat] -outform pem -out /usr/local/share/ca-certificates/openstack.crt

update-ca-certificates
									</programlisting>
									Das öffentliche Zertifikat des OpenStack Authentifizierungs-Endpunktes ist der Konfiguration der OpenStack Instanz zu entnehmen.
									Der entsprechende Wert zum Pfad des Zertifikats ist in der <filename>keystone.conf</filename> unter <emphasis>ca_certs</emphasis> zu finden.
								</entry>
							</row>
							<row>
								<entry>Suchmuster für Images</entry>
								<entry>
									Wenn eine neue virtuelle Maschine erstellt werden soll, werden als Quell-Images nur solche Images angezeigt werden, die dem hier konfigurierten Suchmuster entsprechen.
									Durch den Standardwert "*" (Sternchen) werden alle verfügbaren Images angezeigt.
								</entry>
							</row>
							<row>
								<entry>Projekt / Tenant</entry>
								<entry>
									Der Projekt- oder Tenantname, der dem Benutzer innerhalb der OpenStack Umgebung zugewiesen ist.
								</entry>
							</row>
							<row>
								<entry>Serviceregion</entry>
								<entry>
									Der Name der Region, in der der Benutzer arbeiten soll. Der OpenStack Standardwert ist <emphasis>regionOne</emphasis>.
								</entry>
							</row>
							<row>
								<entry>Servicetyp</entry>
								<entry>
									Der Typ des Dienstes, unter dem die Cloud Compute Funktionalität zur Verfügung steht. Der Standardwert ist <emphasis>compute</emphasis>.
								</entry>
							</row>
							<row>
								<entry>Name des Dienstes</entry>
								<entry>
									Der Name des Dienstes, unter dem die Cloud Compute Funktionalität zur Verfügung steht. Der Standardwert ist <emphasis>nova</emphasis>.
								</entry>
							</row>
							<row>
								<entry>URL des Service-Endpunktes</entry>
								<entry>
									Optionaler Wert: Normalerweise wird die URL des Service-Endpunktes automatisch ermittelt, wenn sich der Benutzer am OpenStack anmeldet. Sollte die automatische Ermittlung nicht möglich sein, kann hier die entsprechende URL angegeben werden.
								</entry>
							</row>
						</tbody>
					</tgroup>
				</table>
			</para>
		</section>
		<section id="uvmm:cloudconnections::amazonec2">
			<title>Anlegen einer EC2 Verbindung</title>
			<para>
				Um eine Verbindung zu Amazon EC2 herzustellen, sind im Einrichtungsassistenten folgende Einstellungen vorzunehmen:
				<table>
					<title>Felder bei der Einrichtung einer Amazon EC2 Verbindung</title>
					<tgroup cols="2">
						<colspec colnum="1" colname="col1" colwidth="1*"/>
						<colspec colnum="2" colname="col2" colwidth="2*"/>
						<thead>
							<row>
								<entry>Attribut</entry>
								<entry>Beschreibung</entry>
							</row>
						</thead>
						<tbody>
							<row>
								<entry>Name</entry>
								<entry>
									Definiert den Namen der Verbindung.
									Dieser wird später in der Baumansicht des &ucsUMC; Moduls angezeigt.
								</entry>
							</row>
							<row>
								<entry>EC2 Region</entry>
								<entry>
									Hier wird ausgewählt, zu welcher EC2 Region die Verbindung aufgebaut werden soll.
									Virtuelle Maschinen sind immer genau einer Region zugeordnet und in anderen Regionen nicht sichtbar.
									Auch die Auswahl der verfügbaren Images kann sich zwischen den Regionen unterscheiden.
									Univention UCS Images sind in allen unterstützten Regionen verfügbar.
								</entry>
							</row>
							<row>
								<entry>Access Key ID</entry>
								<entry>
									Die Zugriffs-ID, die dem Amazon EC2 Konto zugeordnet ist, vergleichbar mit einem Benutzernamen.
								</entry>
							</row>
							<row>
								<entry>Geheimer Zugriffsschlüssel (Secret Access Key)</entry>
								<entry>
									Der geheime Schlüssel zum Zugriff über das Amazon EC2 Konto, vergleichbar mit einem Passwort.
								</entry>
							</row>
							<row>
								<entry>Suchmuster für AMIs</entry>
								<entry>
									Imagedateien als Quelle für neue Instanzen werden als AMI bezeichnet.
									Der hier angegebene Suchfilter beschränkt die Anzeige von auswählbaren AMIs beim anlegen einer neuen virtuellen Instanz.
									Durch den Wert "*" (Sternchen) werden alle verfügbaren Images angezeigt.
								</entry>
							</row>
						</tbody>
					</tgroup>
				</table>
			</para>
		</section>
-->
	</section>

	<section id="uvmm::management">
		<title>Managing virtual machines with &ucsUMC;</title>
		<para>
			The UMC module <emphasis>Virtual machines (UVMM)</emphasis> offers the possibility to create, edit and delete virtual instances/machines and to change their status.

			In principle, these functions are independent of the virtualization technology employed (KVM or cloud-based), however they may vary slightly depending on the hypervisor in use.
			The items that must be observed are illustrated in the following section on the description of the functions.
		</para>

		<section id="uvmm::overview">
			<title>Operations (Starting/stopping/suspending/deleting/migrating/cloning virtual machines)</title>
			<figure id="uvmm-overview">
				<title>Overview of virtual machines</title>
				<graphic scalefit="1" width="80%" fileref="illustrations/uvmm_overview1_en.png"/>
			</figure>
			<para>
				In the main dialog of the UMC module, a tree structure is displayed on the left-hand side, which gives an overview of the existing virtualization servers.
				All the virtual machines are listed in the right half of the screen.
				If one clicks on the name of a virtualization server, only the instances of that server are listed.
				The search mask can also be used to search for individual virtual machines.
			</para>
			<para>
				In the overview of the virtual machines, the computer icon shows the state this is in, e.g., whether it is running (computer symbol with green arrow), paused (computer symbol with yellow line) or stopped (computer without additional symbol).
<!-- TODO: translation
				Virtuelle Maschinen in Cloud Computing Umgebungen können zusätzlich als gelöscht (Rechner mit rotem Kreuz) oder als ausstehend (Rechner mit Sanduhr) dargestellt werden.
-->
			</para>
			<para>
				Instances created via UVMM are turned off in the initial status.
				The icon showing an arrow pointing right can be used to start a virtual instance.
			</para>

			<para>
			  Running instances can be accessed via the VNC protocol - insofar as this is
			  configured. The icon with the stylised screen opens a connection with noVNC, a
			  HTML5-based client. Any other VNC client can also be used for the access; the VNC port
			  is displayed in a tooltip above the computer name.
			</para>

			<para>
				The <guimenu>more</guimenu> choice box can be used to perform other operations:
				The following operations are available on running instances:
			</para>
			<variablelist>
				<varlistentry>
					<term>Stop</term>
					<listitem>
						<simpara>
						turns the virtual machine off.
						It must be noted that the operating system of the virtual machine is not shutdown first, i.e., it should be compared with turning off a computer by pulling the power plug.
						</simpara>
					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Pause</term>
					<listitem>
						<simpara>
						assigns the instance no further CPU time.
						This still uses the working memory on the virtualization server, but the instance itself is paused.
						</simpara>
					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Suspend</term>
					<listitem>
						<simpara>
						saves the contents of the machine's system memory on the hard drive and does not assign the machine further CPU time, i.e., compared with <guimenu>Pause</guimenu> the working memory is also freed.
						This function is only available on KVM-based virtualization servers.
						</simpara>
					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Migrate</term>
					<listitem>
						<simpara>
						migrates the virtual machine to another virtualization server.
						Further information can be found in <xref linkend="uvmm:migration"/>.
						</simpara>
					</listitem>
				</varlistentry>
			</variablelist>
			<para>
				The following operations are available on saved or stopped instances:
			</para>
			<variablelist>
				<varlistentry>
					<term>Remove</term>
					<listitem>
						<simpara>
						Virtual instances no longer required can be deleted along with all their hard drives and ISO images.
						The images to be deleted can be selected from a list.
						It must be noted that ISO images and sometimes also hard drive images may still be used by other instances.
						They should only be deleted when they are no longer used by any instance.
						</simpara>
					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Migrate</term>
					<listitem>
						<simpara>
						migrates the virtual machine to another virtualization server.
						Further information can be found in <xref linkend="uvmm:migration"/>.
						</simpara>
					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Clone</term>
					<listitem>
						<simpara>
						creates a copy of the current VM. It is given a freely selectable,
					new name. Network interfaces are adopted, but can also alternatively be randomly
					regenerated. Mounted CD and DVD drives from the source VM are also integrated in
					the clone, while hard drives are copied insofar as the storage pool supports the
					copying. Snapshots are not copied!
						</simpara>
					</listitem>
				</varlistentry>
			</variablelist>
<!-- TODO: translation
			<para>
				Folgende Operationen stehen für virtuelle Maschinen zur Verfügung, die in cloudbasierten Umgebungen betrieben werden:
			</para>
			<variablelist>
				<varlistentry>
					<term>Neu starten (hard)</term>
					<listitem>
						<simpara>
							Startet die virtuelle Maschine neu, als wäre der Reset-Knopf betätigt worden.
							Hierbei kann es zu Datenverlust kommen.
						</simpara>
					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Neu starten (soft)</term>
					<listitem>
						<simpara>
							Sendet ein ACPI-Reset Event an die virtuelle Maschine.
							Wenn das Betriebssystem der virtuellen Maschine dies korrekt interpretiert, wird ein geordneter Neustart durchgeführt.
						</simpara>
					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Herunterfahren (soft)</term>
					<listitem>
						<simpara>
							Sendet ein ACPI-Shutdown Event an die virtuelle Maschine
							Wenn das Betriebssystem der virtuellen Maschine dies korrekt interpretiert, wird es geordnet heruntergefahren und ausgeschaltet.
						</simpara>
					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Pausieren</term>
					<listitem>
						<simpara>
							Weist der Maschine keine weitere CPU-Zeit zu.
							Dadurch wird weiterhin der Arbeitsspeicher auf dem physikalischen Rechner belegt, die Maschine an sich aber angehalten.
						</simpara>
					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Speichern und beenden</term>
					<listitem>
						<simpara>
							Sichert den Inhalt des Arbeitsspeichers der Maschine auf Festplattenspeicher und weist der Maschine keine weitere CPU-Zeit zu, d.h. gegenüber <guimenu>Pausieren</guimenu> wird außerdem noch der Arbeitsspeicher freigegeben.
						</simpara>
					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Löschen</term>
					<listitem>
						<simpara>
							Schaltet die virtuelle Maschine aus und löscht alle dazugehörigen Daten unwiederruflich.
						</simpara>
					</listitem>
				</varlistentry>
			</variablelist>
		</section>
		<section id="uvmm::cloudinstanz::erstellen">
			<title>Erstellen einer virtuellen Maschine über eine Cloud Verbindung</title>
			<para>
				Virtuelle Maschinen in cloudbasierten Virtualisierungsumgebungen können in UVMM durch Klick auf <guimenu>Erstellen</guimenu> mit einem Assistenten in wenigen Schritten erstellt werden.
			</para>
			<para>
				In der Eingabemaske <guimenu>Erstellen einer virtuellen Maschine oder einer Cloud-Verbindung</guimenu> kann ausgewählt werden, auf über welche Cloud Verbindung die virtuelle Maschine angelegt werden soll.
				Nach der Auswahl einer Verbindung und einem Klick auf <guimenu>Weiter</guimenu>, gelangt man zum Assistenten zum anlegen einer neuen virtuellen Maschine.
				Nach dem Festlegen der Parameter wird die neue virtuelle Maschine nach einem Klick auf <guimenu>Fertigstellen</guimenu> angelegt.
			</para>
			<table>
				<title>Erstellen einer virtuellen Maschine über eine Cloud Verbindung</title>
				<tgroup cols="2">
					<colspec colnum="1" colname="col1" colwidth="1*"/>
					<colspec colnum="2" colname="col2" colwidth="2*"/>
					<thead>
						<row>
							<entry>Attribut</entry>
							<entry>Beschreibung</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>Name</entry>
							<entry>
								Definiert den Namen der virtuellen Maschine.
							</entry>
						</row>
						<row>
							<entry>Auswahl des Quell-Images / Quell-AMIs</entry>
							<entry>
								Der initiale Zustand einer virtuellen Maschine beim anlegen wird über ein Quell-Image (OpenStack) oder Quell-AMI (EC2) festgelegt.
								Ein solches Image enthält meist ein vorbereitetes Betriebssystem, dass vom Nutzer nach dem Start individualisiert werden kann.
								Es können beliebig viele virtuelle Maschinen aus einem Quell-Image erstellt werden.
							</entry>
						</row>
						<row>
							<entry>Wahl der Instanzgröße</entry>
							<entry>
								Einer virtuellen Maschine wird beim anlegen eine Instanzgröße zugeordnet. Diese setzt sich zusammen aus verfügbarem Arbeitsspeicher und der Größe des verfügbaren Festplattenspeichers.
								Beim anlegen einer virtuellen Maschine in einer OpenStack Umgebung wird über die Größe auch die Anzahl der CPU-Kerne bestimmt.
							</entry>
						</row>
						<row>
							<entry>Auswahl eines Schlüsselpaares</entry>
							<entry>
								Um den sicheren Zugriff auf die virtuelle Maschine per ssh zu ermöglichen, wird der Maschine beim ersten Start ein ssh-Schlüssel zur Konfiguration des rootaccounts hinzugefügt.
								Mit diesem Schlüssel ist der ssh Zugriff auf die Maschine ohne Passwort möglich.
								Dazu muss der Zugriff auf den privaten Schlüsselteil des Schlüsselpaares bestehen.
								Der Zugriff auf die Instanz kann z.b. mit folgendem Aufruf erfolgen, wenn die Instanz läuft:
								<programlisting language="sh">
ssh -i [pfad/zum/privaten/schluessel] root@[ip-adresse-der-instanz]
								</programlisting>
							</entry>
						</row>
						<row>
							<entry>Konfigurieren einer Sicherheitsgruppe</entry>
							<entry>
								Diese Einstellung konfiguriert, welche Sicherheitsgruppe für die neue virtuelle Maschine gesetzt wird.
								Eine Sicherheitsgruppe bestimmt, welche TCP-Ports für den externen Zugriff auf eine virtuelle Maschine freigegeben werden.
							</entry>
						</row>
					</tbody>
				</tgroup>
			</table>
		</section>
		<section id="uvmm::cloudinstanz::bearbeiten">
			<title>Bearbeiten einer virtuellen Maschine über eine Cloud Verbindung</title>
			<para>
				Durch Auswahl einer virtuellen Maschine und einem Klick auf <guimenu>Bearbeiten</guimenu> können auf einer separaten Seite die konfigurierten Einstellungen der virtuellen Maschine eingesehen werden.
				Insbesondere die IP-Adresse, über die die virtuelle Maschine erreichbar ist, ist hier einsehbar.
			</para>
-->
		</section>
		<section id="uvmm::instanz::erstellen">
			<title>Creating a virtual instance</title>
			<para>
				Virtual machines on local KVM servers can be created with the assistant in a few steps in UVMM by clicking on <guimenu>Create</guimenu>.
			</para>
			<para>
<!-- TODO: translation
				In der Eingabemaske <guimenu>Erstellen einer virtuellen Maschine oder einer Cloud-Verbindung</guimenu> kann ausgewählt werden, auf welchem Virtualisierungs-Server die virtuelle Maschine angelegt werden soll.
				Wird hier ein KVM Virtualisierungsserver ausgewählt und <guimenu>Weiter</guimenu> gewählt, gelangt man zur Auswahl des Maschinenprofils.
-->
				The selection of the <guimenu>Profile</guimenu> specifies some of the basic settings for the virtual instance (see <xref linkend="uvmm::profile"/>).
			</para>
			<para>
				The virtual machine is now given a <guimenu>Name</guimenu> and an optional <guimenu>Description</guimenu> and assigned <guimenu>Memory</guimenu> and <guimenu>CPUs</guimenu>.

				The <guimenu>Enable direct access</guimenu> option specifies whether the machine can be accessed via the VNC protocol.
				This is generally required for the initial operating system installation.
			</para>
			<para>
				Now the disk drives of the virtual machines are configured.
				The setup is documented in <xref linkend="uvmm:imagefiles"/>.
			</para>
			<para>
				Clicking <guimenu>Finish</guimenu> concludes the creation of the virtual machine.
		</para>
	</section>

	<section id="uvmm-instance-edit">
		<title>Modifying virtual machines</title>
		<para>
			In the overview list, a virtual machine can be edited by clicking on the icon with the stylized pen.
		</para>
		<figure id="uvmm-drive">
			<title>Modifying the settings of a DVD drive</title>
			<graphic scalefit="1" width="30%" fileref="illustrations/uvmm_dvd_en.png"/>
		</figure>
			<table>
				<title>'General' tab</title>
				<tgroup cols="2">
					<colspec colnum="1" colname="col1" colwidth="1*"/>
					<colspec colnum="2" colname="col2" colwidth="2*"/>
					<thead>
						<row>
							<entry>Attribut</entry>
							<entry>Description</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>Name</entry>
							<entry>
								Defines the name of the virtual machine.
								This does not have to be the same as the name of the host in the LDAP directory.
							</entry>
						</row>
						<row>
							<entry>Operating system</entry>
							<entry>
								The operating system installed in the virtual instance.
								Any text can be entered here.
							</entry>
						</row>
						<row>
							<entry>Contact</entry>
							<entry>
								Defines the contact person for the virtual machine.
								If an e-mail address is specified here, an external e-mail program can then be run via the mouseover that appears.
							</entry>
						</row>
						<row>
							<entry>Description</entry>
							<entry>
							  Can be used to describe the function of the virtual machine,
							  e.g. <emphasis>mail server</emphasis> or it's state. The description
							  is shown in the overview of the virtual machines as a mouseover.
							</entry>
						</row>
					</tbody>
				</tgroup>
			</table>

			<para>
			  The tab <guimenu>Devices</guimenu> allows the
			  configuration of drives and network interfaces. An
			  introduction to the supported devices, image formats and
			  storage pools can be found in the <xref
			  linkend="uvmm:imagefiles"/>. An introduction to the
			  supported network card settings can be found in the
			  <xref linkend="uvmm:networkinterfaces"/>.
			</para>
			<para>
				<guimenu>Drives</guimenu> lists all existing drives, the image files used, their size and the assigned storage pools.
				One can click on the stylised minus sign to delete a drive and <guimenu>Edit</guimenu> can be used to adjust setting subsequently.
			</para>
			<para>
				<guimenu>Paravirtual drive</guimenu> allows specification of whether the access to the drive should be paravirtualized.
				Where possible, this setting should not be changed for a virtual machine which already has an operating system installed, as this may disrupt the access of partitions.

			</para>
			<para>
				If drives or network interfaces are subsequently added to a virtual instance, the utilisation of paravirtualization is determined by heuristics or its profile.
			</para>
			<para>
				<guimenu>Add drive</guimenu> can be used to add an additional drive.
			</para>
			<para>
				This menu contains a list of all network cards; in addition, new cards can be added or existing ones edited.
				<guimenu>Add network interface</guimenu> can be used to add another virtual network card.
			</para>

			<para>
			  The tab <guimenu>Snapshots</guimenu> contains a list of all available snapshots.
			  An introduction to snapshots can be found in the <xref linkend="uvmm::snapshots"/>.
			</para>
			<para>
				<guimenu>Snapshots</guimenu> includes a list of all the existing snapshots.
				<guimenu>Resume</guimenu> can be used to restore an earlier status.
			</para>
			<caution>
				<para>
				The current machine state is lost if the old snapshot is restored.
				However, there is no reason not to save the current state in an additional snapshot in advance.
				</para>
			</caution>
			<para>
				A snapshot can be removed by clicking in the stylised minus sign.
				The current state of the virtual machine is not modified by this.
			</para>
			<para>
				<guimenu>Create new snapshot</guimenu> can be used to create a snapshot with the name of your choice, e.g., <emphasis>DC Master before update to UCS 4.0-1</emphasis>.
				In addition to the description the time is saved when the snapshot is created.
			</para>

			<para>
			  The settings of a virtual machine can only be changed if it is turned off.
			</para>

			<table>
				<title>'Advanced' tab</title>
				<tgroup cols="2">
					<colspec colnum="1" colname="col1" colwidth="1*"/>
					<colspec colnum="2" colname="col2" colwidth="2*"/>
					<thead>
						<row>
							<entry>Attribute</entry>
							<entry>Descrition</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>Architecture</entry>
							<entry>
								Specifies the architecture of the emulated hardware.
								It must be noted that virtual 64-bit machines can only be created on virtualization servers using the amd64 architecture.
								This setting is not shown on i386  systems.
							</entry>
						</row>
						<row>
							<entry>Number of CPUs</entry>
							<entry>
								Defines how many CPU sockets are assigned to the virtual instance.
								The number of NUMA nodes, cores and CPU threads is not currently configurable.
							</entry>
						</row>
						<row>
							<entry>Memory</entry>
							<entry>
								Specifies the size of the assigned system memory.
							</entry>
						</row>
						<row>
							<entry>Virtualization technology</entry>
							<entry>
								The technology used for virtualization.
								This setting can only be specified when creating a virtual instance.
							</entry>
						</row>
						<row>
							<entry>RTC reference</entry>
							<entry>
								In fully virtualized systems, a computer clock is emulated for each virtual machine (paravirtualized systems access the clock on the host system directly).
								This option controls the format of the emulated clock;
								it an either be saved in the <guimenu>coordinated universal time (UTC)</guimenu> or the <guimenu>local timezone</guimenu>.
								The use of UTC is recommended for Linux system and the use of the local time zone recommended for Microsoft Windows systems.
							</entry>
						</row>
						<row>
							<entry>Boot order</entry>
							<entry>
								Specifies the order in which the emulated BIOS of the virtual machine searches the drives for bootable media.
								This setting is only available for fully-virtualized instances.
								On paravirtualized machines it is only possible to select one hard drive from which the kernel should be used.
							</entry>
						</row>
						<row>
							<entry>Direct access (VNC)</entry>
							<entry>
							  Defines whether VNC access to the virtual machine is available. If the
							  option is enabled, the virtual machine can be accessed directly via
							  the UMC module using an HTML5-based VNC client or any other VNC client.
							  The VNC URL is displayed in a tool tip.
							</entry>
						</row>
						<row>
							<entry>Globally available</entry>
							<entry>
								This allows VNC access from other systems than the virtualization server.
							</entry>
						</row>
						<row>
							<entry>VNC Password</entry>
							<entry>
								Sets a password for the VNC connection.
							</entry>
						</row>
						<row>
							<entry>Keyboard layout</entry>
							<entry>
								Defines the layout for the keyboard in the VNC session.
							</entry>
						</row>
					</tbody>
				</tgroup>
			</table>
		</section>
	</section>

	<section id="uvmm:kvmfeatures">
		<title>KVM related UVMM features</title>
		<section id="uvmm:imagefiles">
			<title>Image files of virtual machines</title>
			<para>
				If virtual hard drives are added to an instance, <emphasis>image files</emphasis> are usually used for the data keeping.
				An image file can either be generated for this purpose or an existing image file can be assigned to a virtual machine.
				Alternatively, a native block device (hard drive partition, logical volume, iSCSI volume) can be assigned to a virtual machine.
				The direct use of block devices offers performance advantages and is less susceptible to computer crashes.
			</para>
			<para>
				Hard drive images can be administrated in two ways on KVM systems;
				by default images are saved in the <emphasis>Extended format (qcow2)</emphasis>.
				This format supports Copy-on-write which means that changes do not overwrite the original version, but store new versions in different locations.
				The internal references of the file administration are then updated to allow both access to the original and the new version.
				Snapshots can only be created when using hard drive images in <emphasis>Extended format</emphasis>.
				Alternatively, you can also access a hard drive image in <emphasis>Simple format (raw)</emphasis>.
			</para>
			<para>
			  Operating systems use a so-called <emphasis>page cache</emphasis> to accelerate accesses
			  to storage media. If data are accessed which have already been read off a hard drive and
			  these data are still present in the cache, the comparatively slow access to the storage
			  medium is not necessary and the request is answered directly from the page cache.
			</para>

			<para>
			  Write accesses are generally also not directly written on the hard drive, but are usually
			  bundled and, consequently, written more efficiently. However, this involves the risk of
			  data loss, if, for example, a system crashes or the power supply is interrupted. The data
			  which have been only saved in the write cache up to that point and have yet to be
			  synchronised on the storage medium are lost. For this reasons, modern operating systems
			  generally only keep pending write changes for a maximum of several seconds before writing
			  them to the hard drive.
			</para>

			<para>
			  In order to avoid data being stored doubly in the page cache of the host system and also
			  of the guest system, cache strategies can be configured with the
			  <guimenu>Caching</guimenu> option when using KVM, which influence the use of the host
			  system's page cache:
			</para>

			<itemizedlist>
				<listitem>
					<simpara>
				  The default setting since UCS-3.1 is <emphasis>none</emphasis>: in this setting, KVM
				  accesses the hard drive directly and bypasses the page cache on the virtualization
				  server.<!--footnote><para>This strategy does not work on file systems using <literal>tmpfs</literal> or with some <acronym>NFS</acronym> servers.</para></footnote--> Read accesses are answered directly by the hard drive every time and write
				  accesses are passed directly on to the hard drive.
					</simpara>
				</listitem>

				<listitem>
					<simpara>
				  The <emphasis>write-through</emphasis> strategy uses the page cache on the
				  virtualization server, but every write access is also passed on directly to the
				  storage medium. On virtualization servers with a lot of free system memory, read
				  accesses can be more efficient than <emphasis>none</emphasis>. However, the double
				  caching generally has a negative effect on the overall performance.

				  <footnote><para>Instead, it is recommended to make the free memory directly available
				  to the VMs so that they can use the additional memory more efficiently themselves, for
				  instance for caching.</para></footnote>
					</simpara>
				</listitem>

				<listitem>
					<simpara>
				  If the <emphasis>write-back</emphasis> strategy is used, the host's page cache will
				  be used for both read and write accesses. Write accesses are initially only performed
				  in the page cache, before they are then written to the hard drive at a later point in
				  time. In this case, if the host system crashes, data may be lost.
					</simpara>
				</listitem>

				<listitem>
					<simpara>
				  With the <emphasis>unsafe</emphasis> strategy, synchronisation requests sent by the
				  guest system are ignored in order to force the writing of outstanding data on the
				  storage medium explicitly. Compared with <emphasis>write-back</emphasis>, this once
				  again increases the performance, but can result in data loss if the host system
				  crashes. This version is only practical for test systems or comparable installations
				  in which data loss due to the crashing of the host system is not dramatic.
					</simpara>
				</listitem>

				<listitem>
					<simpara>
				  The <emphasis>directsync</emphasis> strategy corresponds to <emphasis>none</emphasis>,
				  with the only difference being that here synchronisation is explicitly forced after
				  every write access.
					</simpara>
				</listitem>

				<listitem>
					<simpara>
				  The <emphasis>Hypervisor default</emphasis> option is dependent on the UCS version and
				  the KVM version with which a guest system was installed: Originally, the standard
				  value until UCS 3.0 was implicitly <emphasis>write-through</emphasis>, but KVM was
				  modified to such an extent with UCS 3.1 that <emphasis>none</emphasis> is now used for
				  all old VMs instead. For VMs resaved with UCS 3.1 the standard value is implicitly
				  <emphasis>write-through</emphasis> again, but new VMs are explicitly saved with
				  <emphasis>none</emphasis>.
					</simpara>
				</listitem>
			</itemizedlist>
			<para>
				If a live migration of virtual machines between different virtualization servers is planned, the storage pool must be stored on a system which can be accessed by all virtualization servers (e.g., an NFS share or an iSCSI target).
				This is described in <xref linkend="uvmm::storagepools"/>.
			</para>
			<para>
				Image files are created as sparse files with the specified size, i.e., these files only grow when they are used and then up to the maximally specified size and thus initially require only minimal disk space.
				As there is a risk here of the disk space being used up during operation, a Nagios monitoring should be integrated, see <xref linkend="nagios::general"/>.
			</para>
<!-- TODO: translation
			<para>
				Festplatten-Images sollten nach Möglichkeit paravirtualisiert angesprochen werden.
				Bei UCS-Systemen, die virtualisiert unter KVM installiert werden, wird durch die Auswahl des UCS-Profils automatisch ein paravirtualisierter Zugriff aktiviert.
				Die Konfiguration von Microsoft Windows-Systemen ist in <xref linkend="uvmm::gplpvvirtio"/> dokumentiert.
			</para>
-->
		</section>

		<section id="uvmm::storagepools">
		  <title>Storage pools</title>

		  <para>
			These image files are stored in so-called storage pools. They can either be stored locally
			on the virtualization server or on a file share. The connection of a storage pool over iSCSI
			is documented in <xref linkend="ext-doc-uvmm"/>.
		  </para>

		  <section id="uvmm::defaultpool">
			<title>Accessing the default storage pool through a file share</title>
			<para>
			  Each virtualization server provides a storage pool with the name
			  <emphasis>default</emphasis> in the standard configuration. It can be found on the
			  virtualization servers in the <filename class="directory">/var/lib/libvirt/images/</filename> directory.
			</para>

			<para>
			  To allow simple access to the storage pool, you can set up a share for the
			  <filename class="directory">/var/lib/libvirt/images/</filename> directory. To do so, you need to create a
			  share with the following options in the UMC module <guimenu>Shares</guimenu>. The share
			  can then be accessed easily from Windows clients via a CIFS network share (or via an NFS
			  mount).
			</para>
			<itemizedlist>
				<listitem>
					<para>General/General settings</para>
					<itemizedlist>
						<listitem><simpara>Name: UVMM-Pool</simpara></listitem>
						<listitem><simpara>Host: The hostname of the UVMM server</simpara></listitem>
						<listitem><simpara>Directory: /var/lib/libvirt/images</simpara></listitem>
						<listitem><simpara>Directory owner, Directory owner group and Directory mode can remain in the default setting</simpara></listitem>
					</itemizedlist>
				</listitem>
				<listitem>
					<para>Advanced settings/Samba permissions</para>
					<itemizedlist>
						<listitem><simpara>Valid users or groups: Administrator</simpara></listitem>
					</itemizedlist>
				</listitem>
			</itemizedlist>
			<para>
			  The image files of a virtual hard drive include all the user data of the virtualized
			  system! The <guimenu>Valid users or groups</guimenu> option ensures that, irrespective
			  of the file system permissions, only the Administrator user can access the share.
			</para>
		  </section>

		  <section id="uvmm::addpool">
			<title>Adding a storage pool</title>
			<para>
			  It is not possible to create an additional storage pool via &ucsUMC;. Instead, this
			  must be done by logging in to the virtualization server as the <emphasis>root</emphasis>
			  user. The following steps are required for this:

			  <itemizedlist>
				<listitem><simpara>
				  The directory in which the data from the storage pool are to be saved must be created;
				  in this case <filename>/mnt/storage</filename>. 
				</simpara></listitem>

				<listitem><simpara>
				  The following command is used to create the new <emphasis>Testpool</emphasis> storage pool:
				  <programlisting language="sh">
	virsh pool-define-as Testpool dir - - - - "/mnt/storage"
				  </programlisting>
				</simpara></listitem>

				<listitem><simpara>
				  The libvirt library used by UVMM differentiates between active and inactive storage
				  pools. To be able to use the storage pool directly, it must be activated:

				  <programlisting language="sh">
	virsh pool-start Testpool
				  </programlisting>

				  The following command ensures that the pool is activated automatically the next time
				  the system is started:

				  <programlisting language="sh">
	virsh pool-autostart Testpool
				  </programlisting>
				</simpara></listitem>
			  </itemizedlist>
			</para>
		  </section>

		  <section id="uvmm::movepool">
			<title>Moving the default storage pool</title>
			<para>
			  To change the underlying file path of the default storage pool at a later point in time,
			  one must log in to the virtualization server as the <emphasis>root</emphasis> user. The
			  following steps are required for this:

			  <itemizedlist>
				<listitem><simpara>
				  The &ucsUCRV; <envar>uvmm/pool/default/path</envar> must be changed to the new directory.
				</simpara></listitem>

				<listitem><simpara>
				  The following commands remove the old storage pool; the pool is changed over to the
				  new path the next time the UVMM is restarted:
				  <programlisting language="sh">
	virsh pool-destroy default
	virsh pool-undefine default
	invoke-rc.d univention-virtual-machine-manager-daemon restart
	invoke-rc.d univention-virtual-machine-manager-node-common restart
				  </programlisting>
				</simpara></listitem>
			  </itemizedlist>
			</para>
		  </section>
		</section>

		<section id="uvmm::drives">
			<title>CD/DVD/floppy drives in virtual machines</title>
			<para>
				CD-/DVD-ROM/floppy drives can be mounted in two ways:
			</para>
			<itemizedlist>
				<listitem>
					<simpara>
					An ISO image can be assigned from a storage pool. If no additional storage pool has
					been created, the files from the pool <emphasis>default</emphasis> are read from the
					directory <filename class="directory">/var/lib/libvirt/images/</filename>.
					</simpara>
				</listitem>
				<listitem>
					<simpara>
					Alternatively, a physical drive from the virtualization server can be connected with the virtual machine.
					</simpara>
				</listitem>
			</itemizedlist>
			<para>
				It is also possible to provide a virtual machine with a disk drive via an image (in VFD format) or the pass-through of a physical drive.
			</para>
			<para>
				If drives are defined for a new virtual machine, it must be ensured that it is possible to boot from the CD-ROM drive.
				The UVMM profile (see <xref linkend="uvmm::profile"/>) specifies the boot order for the fully-virtualized instances in advance.
				For the paravirtualized instances, it is defined by the order on the definition of the drives and can be adapted subsequently in the settings section.
			</para>
		</section>

		<section id="uvmm:networkinterfaces">
			<title>Network interfaces in virtual instances</title>
			<para>
				When a virtual machine is created, it is automatically assigned a network card with a randomly generated MAC address.
				It can be subsequently changed.
			</para>
			<para>
				Two types of network connections are possible:
			</para>
			<itemizedlist>
				<listitem>
					<simpara>
					In the basic settings, a <emphasis>Bridge</emphasis> on the virtualization server is used to access the network directly.
					The virtual machine uses its own IP address and can thus also be reached from other computers.
					</simpara>
				</listitem>
				<listitem>
					<simpara>
					<emphasis>Network Address Translation (NAT)</emphasis> network cards are defined in a private network on the virtualization server.
					To do so, the virtual machine(s) must be assigned an IP address from the 192.168.122.0/24 network.
					This virtual instance is granted the access to the external network via NAT, so that the access is performed via the virtualization server's IP address.
					The virtual machine can thus not be reached from other computers, but can create all outgoing connections itself.
					</simpara>
				</listitem>
			</itemizedlist>
			<figure id="uvmm-network">
				<title>Adding a virtual network interface</title>
				<graphic scalefit="1" width="40%" fileref="illustrations/uvmm_network_en.png"/>
			</figure>
			<para>
				The UVMM servers are already preconfigured for bridging and NAT.
				However, there are restrictions for bridged network cards which are described in <xref linkend="computers:networkcomplex:uvmm"/>.
				For each virtual machine the desired network can be selected through the <guimenu>Source</guimenu> setting.
			</para>
			<para>
				NAT network cards are only restricted by the IP addresses available in the 192.168.122.0/24 network.
			</para>
			<para>
				The <guimenu>Driver</guimenu> can be used to select what type of card will be provided.
				The <emphasis>Realteak RTL-8139</emphasis> is supported by almost all operating systems, the <emphasis>Intel Pro-1000</emphasis> offers advanced abilities and a <emphasis>Paravirtual device</emphasis> offers the best performance.
			</para>
		</section>

		<section id="uvmm::gplpvvirtio">
			<title>Paravirtualization/virtIO drivers for Microsoft Windows systems</title>
<!-- TODO: translation
			<para>
				KVM unterstützt Paravirtualisierung über die virtIO Schnittstelle.
				Durch die Verwendung von Paravirtualisierung können die virtualisierten Systeme einen direkten Zugriff auf die Ressourcen des Virtualisierungsservers erhalten.
				Dies verbessert die Performance erheblich.
				Die Verwendung von Paravirtualisierung/virtIO wird empfohlen.
			</para>
			<para>
				Aktuelle Linux-Systeme unterstützen standardmäßig Paravirtualisierung.
				Mit der Installation der KVM-Pakete werden passende Images bereitgestellt, die dann in der Laufwerksverwaltung in eine virtuelle Maschine eingebunden werden können.
				Die Images werden in den mit der &ucsUCRV; <envar>uvmm/pool/default/path</envar> festgelegten Speicherbereich integriert:
				Auf KVM-Virtualisierungsservern wird ein ISO-Image mit dem Namen <emphasis>KVM Windows drivers</emphasis> bereitgestellt, dass die virtIO-Virtualisierungstreiber für Microsoft Windows enthält.
			</para>
-->
			<section id="uvmm:virtio">
				<title>Installation of the virtIO drivers for KVM instances</title>
				<para>
					In Windows systems installed under KVM, paravirtualization must be activated <emphasis>before</emphasis> beginning the Windows installation.
				</para>
				<para>
					The virtIO interface allows the efficient usage of network and storage resources for a virtual machine on the KVM hypervisor.
					The following steps describe the installation of the virtIO drivers on Windows 7.
				</para>
				<itemizedlist>
					<listitem>
						<simpara>
						A CD/DVDy drive needs to be setup in the drive settings with the image <emphasis>KVM Windows drivers (virtio 1.1.65)</emphasis> assigned.
						</simpara>
					</listitem>
					<listitem>
						<simpara>
						The hard disk drive has to be edited in the <guimenu>Devices</guimenu> menu in UVMM and the checkbox <guimenu>Paravirtual drive</guimenu> must be ticked.
						</simpara>
					</listitem>

					<listitem><simpara>
					  The <guimenu>Driver</guimenu> must be configured to <emphasis>Paravirtual device
					  (virtio)</emphasis> for the network card(s).
					</simpara></listitem>

					<listitem>
						<simpara>
						The initial steps during the installation of the Windows system take place as usual.
						A warning appears during hard disk partitioning and states that no mass storage could be found.
						This is not an error because the virtIO drivers are necessary for a paravirtualized device.
						The virtIO drivers can be installed in the same menu with <guimenu>Load drivers</guimenu>.
						The <emphasis>Red Hat virtIO SCSI Controller</emphasis> has to be chosen for Windows 7 (and for Windows 2003 and Windows 2008 respectively) and the <emphasis>Red Hat virtIO Ethernet Adapter</emphasis> for Windows 2008/Windows 7.
						After the device drivers have been installed, the mass storage is available in the Windows installer and the installation of Microsoft Windows can be continued.
						</simpara>
					</listitem>
					<listitem>
						<simpara>
						After completing the installation the devices <emphasis>Red Hat virtIO SCSI Disk Device</emphasis> and <emphasis>Red Hat virtIO Ethernet Adapter</emphasis> can be found in the Windows device manager.
						</simpara>
					</listitem>
				</itemizedlist>
			</section>
		</section>

		<section id="uvmm::snapshots">
			<title>Snapshots</title>
			<para>
				UVMM offers the possibility to save the contents of the main and hard drive memory of a virtual machine in snapshots.
				This allows the administrator to revert to these snapshots at a later point in time, which makes them a useful "safety net" when installing software updates.
			</para>
			<para>
				Snapshots can only be used with KVM instances which access all their hard drive images in Qcow2 format.
				All snapshots are stored using copy-on-write (see <xref linkend="uvmm::instanz::erstellen"/>) directly in the hard drive image file.
			</para>
		</section>

		<section id="uvmm:migration">
			<title>Migration of virtual instances</title>
			<para>
				UVMM offers the the possibility of migrating a virtual machine to another virtualization server.
				This works with both paused and running instances (live migration).
				The option is only offered if at least two compatible virtualization servers are available in the domain.
			</para>
			<figure id="uvmm-migrate">
				<title>Migrating a virtual instance</title>
				<graphic scalefit="1" width="40%" fileref="illustrations/uvmm_migrate_en.png"/>
			</figure>
			<para>
				During the migration it must be noted that the images of the mounted hard drives and CD-ROM drive must be accesible by both virtualization servers.
				This can be achieved, for example, by storing the images in a central storage system.
				Notes on the setting up of this type of environment can be found under <xref linkend="uvmm::storagepools"/>.
			</para>
			<section id="uvmm:Migration_of_virtual_machines_from_failed_virtualization_servers">
				<title>Migration of virtual machines from failed virtualization servers</title>
				<para>
					Information about the virtual machines running on the virtualization servers is stored centrally in the &ucsUVMM;.
					If a server fails (failure detection is performed periodically every 15 seconds), the server and the virtual instances operated on it are identified as unaccessible with a red symbol, a warning appears and <guimenu>Migrate</guimenu> is offered as the only operation in the menu.
				</para>
				<para>
					Following the migration, the virtual instance is no longer displayed in the overview tree of the failed virtualization server in the UVMM.
				</para>
				<caution>
					<para>
					It must be ensured under all circumstances that the virtual machine on the original and the secondary server are not started in parallel; this would involve their both writing in the image files simultaneously, which would result in data loss.
					If virtual machines are started automatically after startup, simultaneous access must be prohibited by disconnecting the network connection or restricting access to the storage pool.
					</para>
				</caution>
				<para>
					If the failed computer is reactivated - e.g., in the case of a temporary power failure - the virtual machines remain available on the system locally and are reported to UVMM; consequently, there are then two versions of the instance.
				</para>
				<para>
					As such, one of the two instances should subsequently be deleted.
					However, the employed image files for the drives should <emphasis>not</emphasis> be deleted at the same time.
				</para>
			</section>
		</section>
	</section>

	<section id="uvmm::profile">
		<title>Profiles</title>
		<para>
			Profiles are used to store initial settings when creating new virtual machines.
			Amongst others this includes the following settings:
		</para>
		<itemizedlist>
			<listitem><simpara>name prefix for new virtual machines</simpara></listitem>
			<listitem><simpara>number of virtual <acronym>CPU</acronym>s</simpara></listitem>
			<listitem><simpara>default RAM size</simpara></listitem>
			<listitem><simpara>default size for new disk images</simpara></listitem>
			<listitem><simpara>default boot order for fully-virtualized virtual machines</simpara></listitem>
			<listitem><simpara>use of paravirtual device drivers</simpara></listitem>
			<listitem><simpara>default settings for direct access per <acronym>VNC</acronym></simpara></listitem>
			<listitem><simpara>network bridge name</simpara></listitem>
		</itemizedlist>
		<para>
			The existing UVMM profiles are stored in the <acronym>LDAP</acronym> directory and can also be edited there.
			The profiles can be found in the UMC module <guimenu>LDAP directory</guimenu> in the container <emphasis>cn=Profiles,cn=Virtual Machine Manager</emphasis>.
			Additional profiles can also be added there.
		</para>
		<section id="uvmm::profile::network">
			<title>Changing default network</title>
			<para>
				The name of the bridge used as the default network interface is stored in <acronym>UVMM</acronym> profiles.
				If the default interface <filename class="device">br0</filename> is changed, the name should be updated as well.
				The following command updates all profiles currently using interface <filename class="device">br0</filename> to use the bridge <filename class="device">eth0</filename>:
			</para>
			<programlisting language="sh">
udm uvmm/profile list --filter interface=br0 |
	sed -ne 's/^DN: //p' |
	xargs -r -d '\n' -n 1 udm uvmm/profile modify --set interface=br0 --dn
			</programlisting>
		</section>
	</section>
</chapter>
